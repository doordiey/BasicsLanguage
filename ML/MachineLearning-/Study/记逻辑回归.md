# 逻辑回归

## 逻辑回归的相关概念：

`根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类。`

- 回归概念

  用一条直线将一些点拟合，这个拟合的过程就叫回归。

  回归一词指的是我们跟据之前的数据预测出一个准确的输出值．

- sigmoid函数

  该函数不光能接受所有的输入然后预测处类别，还能更好的处理函数在跳跃点上从０瞬间跳跃到１。

- 梯度上升与梯度下降法

  就相当于上坡与下坡的过程，不断的找最短路径，没在一个点都看看哪个位置的变化最大的方向，向那个发表方向前进（前进多少距离被称为歩长，可以自定义，步长视具体情况而定。（求最大值为）

- 局部最优现象

  可能梯度下降的最终点不是全局最小点，而只是一个局部最小点。

  

##第一个问题：使用 Logistic 回归在简单数据集上的分类

#### ①准备数据

```python
def loadDataSet(file_name):
    dataMat = []
    labelMat = []
    fr = open(file_name)
    for line in fr.readlines():
        lineArr = line.strip().split()//将数据提取出来
        dataMat.append([1.0,float(lineArr[0]),float(lineArr[1])])
        labelMat.append(int(lineArr[2]))//构建成便于进一步处理的格式
    return dataMat,labelMat
//准备好函数
def sigmoid(inX): #将输入的值转化为０－１之间的值，且当输入值越大时，输出值越接近１，输入值越小越接近０．使用该函数来归一化处理。可以使回归的速度快一些。
   #例如：(负数同理)
#>>> sigmoid(1)
#0.7615941559557646
#>>> sigmoid(2)
#0.9640275800758169
#>>> sigmoid(3)
#0.9950547536867307
#>>> sigmoid(10)
#0.9999999958776926
#>>> sigmoid(100)
#1.0

    return 2 * 1.0/(1+exp(-2*inX)) - 1

```

#### ②数据处理

```python
def gradAscent(dataMatIn,classLabels):(遍历整个数据集进行计算)
    dataMatrix = mat(dataMatIn) //将数据矩阵化
    labelMat = mat(classLabels).transpose()　//矩阵化的同时进行转置．
    m,n = shape(dataMatrix)// m->数据量，样本数 n->特征数
    alpha = 0.001 //步长(过大可能会导致不断偏离最值，过小会导致递进次数需要耗费很多)
    maxCycles = 500　//次数
    weights = ones((n,1))
    for k in range(maxCycles):
        h = sigmoid(dataMatrix*weights)//同一化处理
        error = (labelMat - h)　//误差情况
        //根据误差情况，对权值进行调整，就相当于往一个方向走了一步后根据走偏的方向进行一定程度的调整，调整幅度与步长关系大，调整程度与循环次数有一定关系。
        weights = weights + alpha * dataMatrix.transpose() * error
        //weights的公式的后面部分是由公式推导而来，即alpha * dataMatrix.transpose() * error为我们所求的梯度。关于这一步为梯度的理解：f(w) = w的转置　×　x　计算梯度，对f(w)关于w求导，再根据矩阵求导法则。即线性回归中的求导数。（导数决定了变化的方向是正还是负。）
    return array(weights)

```

#### ③画图(将数据在坐标轴上标点，并将拟合直线也画出来。)

```python
def plotBestfit(dataArr,labelMat,weights):
    n = shape(dataArr)[0]
    xcord1 = []
    ycord1 = []
    xcord2 = []
    ycord2 = []
    for i in range(n):
        if int(labelMat[i]) == 1:
            xcord1.append(dataArr[i,1])
            ycord1.append(dataArr[i,2])
        else:
            xcord2.append(dataArr[i,1])
            ycord2.append(dataArr[i,2])
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(xcord1,ycord1,s=30,c='red',marker = 's')
    ax.scatter(xcord2,ycord2,s=30,c='green')
    x = arange(-3.0,3.0,0.1)
    //此处计算y的公式的一些说明：
    y = (-weights[0]-weights[1]*x)/weights[2]
    ax.plot(x,y)
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.show()

```

#### ④分析结果与改进

```python
def testLR():
    dataMat,labelMat = loadDataSet("TestSet.txt")
    dataArr = array(dataMat)
    weights = gradAscent(dataArr,labelMat)
    plotBestfit(dataArr,labelMat,weights)

#尽管将步长和次数尽可能调大，发现其输出图形拟合度很差。说明需要进一步的优化改进算法本身。
```

##### 关于运算过程的改进的想法：（可能只是一些瞎想

１．歩长动态变化。（就类似于，下坡遇到不太确定的方向就算要走也走的慢一点，确定的时候就大步走过去。）

#### 看到的资料中提到的关于改进的两种实现方法：

##### 随机梯度上升算法：(只用一个样本点来更新回归系数）

```python
def stocGradAscent0(dataMatrix,classLabels):
    m,n = shape(dataMatrix)
    alpha = 0.000001
    weights = ones(n)
    for i in range(m):
        h = sigmoid(sum(dataMatrix[i]*weights))
        error = classLabels[i] - h
        weights = weights + alpha * error * dataMatrix[i]
    return weights
```

#### 随机梯度上升算法（随机化）

```python
def stocGradAscent1(dataMatrix,classLabels,numIter=150):
    m,n = shape(dataMatrix)
    weights = ones(n)
    for j in range(numIter):
        dataIndex = range(m)
        for i in range(m):
            alpha = 4/(1.0+j+i)+0.000001 //此处步长为动态变化
            randIndex = int(random.uniform(0,len(dataIndex)))
            h = sigmoid(sum(dataMatrix[dataIndex[randIndex]]*weights))
            error = classLabels[dataIndex[randIndex]] - h
            weights = weights + alpha * error * dataMatrix[dataIndex[randIndex]]
            del(dataIndex[randIndex])
    return weights


```

### 可以配合着吴恩达的机器Ｍachine Learning课程的关于Model and Cost Function，以及Gradient Descent的基本内容，如果对矩阵运算不熟悉的，其系列视频也有相关介绍，以及有多个特征值的回归公式的推导及解释。理解回归概念的理论，及公式推导的相关。(以下为相关学习视频的链接)

#### [Cost function](https://www.coursera.org/learn/machine-learning/lecture/rkTp3/cost-function)

#### [gradient-descent](https://www.coursera.org/learn/machine-learning/lecture/8SpIM/gradient-descent)

