# Day 1:

## 数据预处理

[数据预处理详解](<https://github.com/doordiey/Python_data_analysis/blob/master/study/%E8%AE%B0%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.md>)

- 第一步：引入需要的库
  - NumPy 包含许多数学操作
  - Pandas 使用这个库可以更好的引入和管理数据集
- 第二步：引入数据集
  - 数据集通常是.csv文件。一个CSV文件可以以纯文本形式存储表格数据。文件中的每一行代表一个数据记录。我们可以使用pandas库中的read_csv方法去加载本地的csv文件成一个数据文件，然后使用不同的矩阵和向量分离出独立和不独立的变量。
- 第三步：处理错误数据 【清洗数据】
  - 我们得到的初始数据是十分巨大的，数据可能因为各种原因造成丢失，我们需要解决这样的问题以免对我们的机器学习模型的表现造成影响。我们可以用整列的平均值或中值替代缺失的值。例子中用了sklearn.proprocessing。
- 第四步：分类数据
  - 分类数据是包含标签值而不是数值的变量。可能的值的数量是限定在一定集合内的，比如一个值是是和不是不可以与模型内的数值匹配，我们就需要去把这些变量定义为对应的数值。为了做到这一点，我们引入了sklearn.preprocessing 中的labelEncoder
- 第五步：把数据集内的数据划分为测试集和训练集
  - 我们需要两部分的数据集一部分用于去训练模型而另一部分去测试训练后模型的表现。我们经常以80：20的比例划分。为此引入了sklearn.crossbalidation库中的train_test_split()
- 第六步：特征缩放   【归一化】
  - 大部分的机器学习算法使用两个数据点的欧几里德距离进行计算。当不同特征的值的数据量级差距过大会导致问题，高量级的特征会在距离计算中占很大的比重。为了解决这个问题，用了sklearn.preprocessing中的StandarScalar库。

